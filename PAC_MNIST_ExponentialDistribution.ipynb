{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PGM-Lab/PAC2BAYES/blob/master/PAC_MNIST_ExponentialDistribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2EOnCShVSJ4"
      },
      "source": [
        "# Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXsO6OyvFiJz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(tf.test.gpu_device_name())\n",
        "tf.keras.backend.clear_session()\n",
        "tf.config.optimizer.set_jit(True) # Enable XLA."
      ],
      "metadata": {
        "id": "ezfO-JPQ9JNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "Nyjb_iYxcdCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "train_size = 1000\n",
        "\n",
        "def load_data(data_set, train_size):\n",
        "  (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "  #(x_test, y_test), (x_train, y_train) = datasets.mnist.load_data()\n",
        "  \n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  a = np.random.permutation(train_size)\n",
        "  x_train = x_train[a,...]\n",
        "  y_train = y_train[a]\n",
        "  # Add a new axis\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  # Data normalization\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "SXIhRtCfxCFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Fashion MNIST"
      ],
      "metadata": {
        "id": "t5_Bu_zpzEeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = load_data('mnist', train_size)"
      ],
      "metadata": {
        "id": "aqtta5IZyE-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training samples:\", x_train.shape[0])\n",
        "print(\"Test samples:\", x_test.shape[0])"
      ],
      "metadata": {
        "id": "9I9Xh5YHGy_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "bdCMIElKciEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import warnings\n",
        "\n",
        "class EarlyStoppingByLossVal(Callback):\n",
        "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
        "        super(Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
        "\n",
        "        if current < self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "idKC7T4qmHPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Flatten\n",
        "\n",
        "def get_lenet(l2_reg = 0.0):\n",
        "    tf.keras.utils.set_random_seed(123)\n",
        "\n",
        "    LeNet_l2 = Sequential()\n",
        "    LeNet_l2.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28, 28, 1), \n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(MaxPool2D(strides=2))\n",
        "    LeNet_l2.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu', \n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(MaxPool2D(strides=2))\n",
        "    LeNet_l2.add(Flatten())\n",
        "    LeNet_l2.add(Dense(256, activation='relu', \n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(Dense(84, activation='relu',\n",
        "                kernel_regularizer=tf.keras.regularizers.L2(l2_reg),\n",
        "                bias_regularizer=tf.keras.regularizers.L2(l2_reg)))\n",
        "    LeNet_l2.add(Dense(10))\n",
        "\n",
        "    LeNet_l2.build()\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True\n",
        "    )\n",
        "\n",
        "\n",
        "    adam = Adam(learning_rate=1e-3)\n",
        "    LeNet_l2.compile(loss=cce, metrics=[cce, 'accuracy'], optimizer=adam)\n",
        "    return LeNet_l2"
      ],
      "metadata": {
        "id": "NosJNCQebjFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Set-up"
      ],
      "metadata": {
        "id": "iwGNoPYQQdNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_norm(model):\n",
        "    norm = []\n",
        "    for w in model.get_weights():\n",
        "        norm.append(tf.norm(w))\n",
        "    return np.sum(norm)\n",
        "\n",
        "def model_relative_norm(model, model_reference):\n",
        "    norm = []\n",
        "    for i in range(len(model.get_weights())):\n",
        "        norm.append(tf.norm(model.get_weights()[i]-model_reference.get_weights()[i]))\n",
        "    return np.sum(norm)    "
      ],
      "metadata": {
        "id": "Suu7_-Fxt86O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, epochs, batch_size=train_size):\n",
        "    callbacks = [EarlyStoppingByLossVal(monitor='categorical_crossentropy', value=0.05, verbose=1)]\n",
        "    model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, callbacks = callbacks, verbose = 0)\n",
        "    train_metrics = model.evaluate(x_train, y_train)\n",
        "    test_metrics = model.evaluate(x_test, y_test)\n",
        "    return train_metrics, test_metrics"
      ],
      "metadata": {
        "id": "LAF9sxkfg8Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_jensen(model, lambdas):\n",
        "    cce_red = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    y_pred = model.predict(x_test)\n",
        "    log_p = -cce_red(y_test, y_pred)\n",
        "    return np.array([(tfp.math.reduce_logmeanexp(lamb * log_p) - tf.reduce_mean(lamb * log_p)) for lamb in lambdas])\n",
        "\n",
        "def jensen_function(model, lamb):\n",
        "    cce_red = tf.keras.losses.CategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    y_pred = model.predict(x_test)\n",
        "    log_p = -cce_red(y_test, y_pred)\n",
        "    return (tfp.math.reduce_logmeanexp(lamb * log_p))/lamb, tf.reduce_mean(log_p), (tfp.math.reduce_logmeanexp(lamb * log_p) - tf.reduce_mean(lamb * log_p))/lamb\n",
        "\n",
        "\n",
        "\n",
        "def rate_function(model, lambdas, s_values):\n",
        "  jensen_vals = eval_jensen(model, lambdas)\n",
        "  def temp(s):\n",
        "    if s>0:\n",
        "      return np.max(lambdas[lambdas>0]*np.abs(s) - jensen_vals[lambdas>0])\n",
        "    else:\n",
        "      return np.max((-lambdas[lambdas<0])*np.abs(s) - jensen_vals[lambdas<0])\n",
        "  \n",
        "  return [temp(s) for s in s_values]\n",
        "\n",
        "def rate_function_positive(model, lambdas, s_values):\n",
        "  jensen_vals = eval_jensen(model, lambdas)\n",
        "\n",
        "  return [ np.max(lambdas*s - jensen_vals) for s in s_values]\n",
        "\n",
        "\n",
        "def rate_function_negative(model, lambdas, s_values):\n",
        "  jensen_vals = eval_jensen(model, -lambdas)\n",
        "\n",
        "  return [ np.max(lambdas*s - jensen_vals) for s in s_values]\n",
        "\n",
        "\n",
        "def inverse_rate_function_alpha(model, lambdas, alpha_vals):\n",
        "  jensen_vals = eval_jensen(model, lambdas)\n",
        "\n",
        "  out = []\n",
        "  for alpha in alpha_vals:\n",
        "    vec = (jensen_vals + alpha)/lambdas\n",
        "    index = np.argmin(vec)\n",
        "    if index==len(lambdas)-1:\n",
        "      raise Exception('Not enough high lambda range')\n",
        "\n",
        "    out.append(vec[index])\n",
        "  return out\n",
        "\n",
        "\n",
        "def inverse_rate_function_alpha_negative(model, lambdas, alpha_vals):\n",
        "  jensen_vals = eval_jensen(model, -lambdas)\n",
        "\n",
        "  out = []\n",
        "  for alpha in alpha_vals:\n",
        "    vec = (jensen_vals + alpha)/lambdas\n",
        "    index = np.argmin(vec)\n",
        "    if index==len(lambdas)-1:\n",
        "      raise Exception('Not enough high lambda range')\n",
        "\n",
        "    if index==0:\n",
        "      raise Exception('Not enough low lambda range')\n",
        "\n",
        "    out.append(vec[index])\n",
        "  return out\n",
        "\n",
        "\n",
        "def inverse_rate_function(model, model_reference, lambdas, n_samples):\n",
        "  jensen_vals = eval_jensen(model, lambdas)\n",
        "  model_class_capacity = np.log((model_relative_norm(model, model_reference)+0.05)/0.05)*model.count_params()/n_samples\n",
        "\n",
        "  inverse_rate_vals = (jensen_vals+model_class_capacity+np.log(lambdas.size/0.05))/lambdas\n",
        "\n",
        "  return np.min(inverse_rate_vals), lambdas[np.argmin(inverse_rate_vals)]\n",
        "\n",
        "def model_weights(model):\n",
        "    norm = []\n",
        "    for w in model.get_weights():\n",
        "        norm.append(tf.reshape(w,[-1]))\n",
        "\n",
        "    w_vals = []\n",
        "    for w in norm:  \n",
        "      w_vals.append(tf.reshape(w,[-1]))\n",
        "\n",
        "    return tf.concat(w_vals,axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "EijCIDEdhRkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l2_values = [0.0, 0.0, 0.01, 0.01]\n",
        "bach_sizes = [train_size, 50, train_size, 50]\n",
        "labels = ['GD', 'SGD', 'GD+L2', 'SGD+L2']\n",
        "#l2_values = [0.0]"
      ],
      "metadata": {
        "id": "40m6Zz2tilei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "rtHOok7g1guT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [get_lenet(l2) for l2 in l2_values]"
      ],
      "metadata": {
        "id": "iIEGu98ih32p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [train_and_evaluate(models[i], 200, batch_size=bach_sizes[i]) for i in range(len(models))]"
      ],
      "metadata": {
        "id": "uQZ26Mj1iBOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Jensen-Gap and Rate Functions"
      ],
      "metadata": {
        "id": "mO9wNXYb1kwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norms = [model_norm(model) for model in models]\n",
        "norms"
      ],
      "metadata": {
        "id": "g-ODU0FUt2hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = np.linspace(-0.5, 0.5, 100)\n",
        "jensens = [eval_jensen(model, lambdas) for model in models]"
      ],
      "metadata": {
        "id": "ICpmAk50iQPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ],
      "metadata": {
        "id": "3CGajG9PdQGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in np.arange(len(l2_values)):\n",
        "    plt.plot(lambdas, jensens[i], label = \"{}\".format(labels[i]))\n",
        "    plt.ylim(0,0.1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OGwPVA-cdYM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling $\\alpha$ for a given model using data sets of size $batch\\_size$"
      ],
      "metadata": {
        "id": "g5TgxZ87Jvar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We fix the i-th\n",
        "i=1\n",
        "batch_size=50\n",
        "\n",
        "\n",
        "batch_vals = []\n",
        "\n",
        "#L(\\theta)\n",
        "eval_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "eval_dataset = eval_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "metric_test = models[i].evaluate(x_test, y_test, batch_size=batch_size)\n",
        "L=metric_test[1]\n",
        "\n",
        "for step, (x_batch, y_batch) in enumerate(eval_dataset):\n",
        "  #\\hatL(D_i,\\theta)\n",
        "  metric_batch = models[i].evaluate(x_batch, y_batch, batch_size=batch_size, verbose=False)\n",
        "  batch_vals.append(metric_batch[1])\n",
        "\n",
        "batch_vals = np.array(batch_vals)\n"
      ],
      "metadata": {
        "id": "uqkNkQARfHEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(L)\n",
        "print(np.mean(batch_vals))\n",
        "plt.hist(batch_vals, bins=20)\n",
        "batch_vals.shape"
      ],
      "metadata": {
        "id": "63Jxualsmo95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(batch_vals)"
      ],
      "metadata": {
        "id": "cFwvvp-wplJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = np.arange(-5, 5, 0.01)\n",
        "\n",
        "total_alpha_vals = rate_function(models[i],lambdas,L-batch_vals)\n",
        "total_alpha_vals = batch_size*np.array(total_alpha_vals)*np.sign(L-batch_vals)\n",
        "np.sum(total_alpha_vals)"
      ],
      "metadata": {
        "id": "pxtWxCeoiLbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to distinguish between positive and negative alphas"
      ],
      "metadata": {
        "id": "KGD-Emk8LMCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_vals = total_alpha_vals[L>batch_vals]\n",
        "print(len(alpha_vals))\n",
        "alpha_vals = alpha_vals[alpha_vals>np.percentile(alpha_vals, 20)]#-0.05\n",
        "len(alpha_vals)"
      ],
      "metadata": {
        "id": "y2Keo_EjhU-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lambda_rate = 1#/np.mean(alpha_vals) #batch_size\n",
        "print(1/np.mean(alpha_vals))\n",
        "\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(alpha_vals, bins = 20, density=True)\n",
        "#plt.hist(data, bins=30, density=True, alpha=0.5)\n",
        "\n",
        "# Plot density\n",
        "x = np.linspace(0, np.log(1.0/0.0001)/lambda_rate, 1000)\n",
        "y = lambda_rate *np.exp(-lambda_rate * x)\n",
        "plt.plot(x, y, color='red')\n",
        "#plt.plot(-x, y, color='red')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Exponential Distribution')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SWLQUZtzkLOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, kurtosis\n",
        "print(skew(alpha_vals))\n",
        "print(kurtosis(alpha_vals))"
      ],
      "metadata": {
        "id": "RfdKKBnfucCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import expon, kstest, anderson\n",
        "\n",
        "# Define rate parameter\n",
        "rate = 1#/np.mean(alpha_vals)\n",
        "print(1/np.mean(alpha_vals))\n",
        "\n",
        "\n",
        "# Perform Kolmogorov-Smirnov test\n",
        "ks_statistic, p_value = kstest(alpha_vals, expon.cdf, args=(0, 1/rate))\n",
        "print('KS test statistic:', ks_statistic)\n",
        "print('KS test p-value:', p_value)\n",
        "\n",
        "# Perform Anderson-Darling test\n",
        "ad_statistic, critical_values, significance_levels = anderson(alpha_vals, 'expon')\n",
        "print('AD test statistic:', ad_statistic)\n",
        "print('AD critical values:', critical_values)\n",
        "print('AD significance levels:', significance_levels)\n"
      ],
      "metadata": {
        "id": "URh9dTx6k6jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YoAnOJ7WzUSC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}